{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully logged in to Hugging Face!\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "# 设置你的 Hugging Face API Token\n",
    "hf_token = \"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\"\n",
    "\n",
    "# 登录 Hugging Face，设置 API Token\n",
    "login(hf_token)\n",
    "\n",
    "print(\"Successfully logged in to Hugging Face!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine tune model Finance chat with Fiqa dataset\n",
    "\n",
    "* Model information:\n",
    "    - Model name: [AdaptLLM/finance-chat](https://huggingface.co/AdaptLLM/finance-chat)\n",
    "    - Description: the domain-specific chat model developed from LLaMA-2-Chat-7B, using the method in our ICLR 2024 paper Adapting Large Language Models via Reading Comprehension.\n",
    "    - List dataset used to train:\n",
    "        - [Open-Orca/OpenOrca](https://huggingface.co/datasets/Open-Orca/OpenOrca)\n",
    "        - [GAIR/lima](https://huggingface.co/datasets/GAIR/lima)\n",
    "        - [WizardLM/WizardLM_evol_instruct_V2_196k](https://huggingface.co/datasets/WizardLM/WizardLM_evol_instruct_V2_196k)\n",
    "* Dataset information:\n",
    "    - Dataset name: [FinGPT/fingpt-fiqa_qa](https://huggingface.co/datasets/FinGPT/fingpt-fiqa_qa)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install packages, setup global settings\n",
    "### 1.1 Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (2.5.0)\n",
      "Requirement already satisfied: filelock in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: bitsandbytes in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (0.44.1)\n",
      "Requirement already satisfied: torch in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from bitsandbytes) (2.5.0)\n",
      "Requirement already satisfied: numpy in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from bitsandbytes) (2.1.2)\n",
      "Requirement already satisfied: filelock in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch->bitsandbytes) (3.16.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch->bitsandbytes) (4.11.0)\n",
      "Requirement already satisfied: networkx in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch->bitsandbytes) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch->bitsandbytes) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch->bitsandbytes) (2023.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch->bitsandbytes) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch->bitsandbytes) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch->bitsandbytes) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch->bitsandbytes) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch->bitsandbytes) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch->bitsandbytes) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch->bitsandbytes) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch->bitsandbytes) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch->bitsandbytes) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch->bitsandbytes) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from jinja2->torch->bitsandbytes) (2.1.3)\n",
      "Requirement already satisfied: transformers in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (4.46.0)\n",
      "Requirement already satisfied: peft in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (0.4.0)\n",
      "Requirement already satisfied: accelerate in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (1.0.1)\n",
      "Requirement already satisfied: trl in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (0.11.4)\n",
      "Requirement already satisfied: filelock in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from transformers) (3.16.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from transformers) (0.26.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from transformers) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from transformers) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
      "Requirement already satisfied: requests in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from transformers) (2.32.3)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from transformers) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from transformers) (4.66.5)\n",
      "Requirement already satisfied: psutil in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from peft) (2.5.0)\n",
      "Requirement already satisfied: datasets in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from trl) (2.16.1)\n",
      "Requirement already satisfied: tyro>=0.5.11 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from trl) (0.8.12)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2023.10.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: networkx in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.4.1)\n",
      "Requirement already satisfied: jinja2 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.1.4)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (9.1.0.70)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.4.5.8)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (11.2.1.3)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (10.3.5.147)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (11.6.1.9)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.3.1.170)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (12.4.127)\n",
      "Requirement already satisfied: triton==3.1.0 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (3.1.0)\n",
      "Requirement already satisfied: setuptools in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from torch>=1.13.0->peft) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: docstring-parser>=0.16 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from tyro>=0.5.11->trl) (0.16)\n",
      "Requirement already satisfied: rich>=11.1.0 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from tyro>=0.5.11->trl) (13.9.2)\n",
      "Requirement already satisfied: shtab>=1.5.6 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from tyro>=0.5.11->trl) (1.7.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from datasets->trl) (17.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from datasets->trl) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from datasets->trl) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from datasets->trl) (2.2.3)\n",
      "Requirement already satisfied: xxhash in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from datasets->trl) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from datasets->trl) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from datasets->trl) (3.10.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from requests->transformers) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from aiohttp->datasets->trl) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from aiohttp->datasets->trl) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from aiohttp->datasets->trl) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from aiohttp->datasets->trl) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from aiohttp->datasets->trl) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from aiohttp->datasets->trl) (1.15.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from rich>=11.1.0->tyro>=0.5.11->trl) (2.15.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from pandas->datasets->trl) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from pandas->datasets->trl) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from pandas->datasets->trl) (2024.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl) (0.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets->trl) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets->trl) (0.2.0)\n",
      "Requirement already satisfied: datasets==2.16.1 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (2.16.1)\n",
      "Requirement already satisfied: filelock in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from datasets==2.16.1) (3.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from datasets==2.16.1) (2.1.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from datasets==2.16.1) (17.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from datasets==2.16.1) (0.6)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from datasets==2.16.1) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from datasets==2.16.1) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from datasets==2.16.1) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from datasets==2.16.1) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from datasets==2.16.1) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from datasets==2.16.1) (0.70.15)\n",
      "Requirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.16.1) (2023.10.0)\n",
      "Requirement already satisfied: aiohttp in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from datasets==2.16.1) (3.10.10)\n",
      "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from datasets==2.16.1) (0.26.0)\n",
      "Requirement already satisfied: packaging in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from datasets==2.16.1) (24.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from datasets==2.16.1) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from aiohttp->datasets==2.16.1) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from aiohttp->datasets==2.16.1) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from aiohttp->datasets==2.16.1) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from aiohttp->datasets==2.16.1) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from aiohttp->datasets==2.16.1) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from aiohttp->datasets==2.16.1) (1.15.5)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from huggingface-hub>=0.19.4->datasets==2.16.1) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from requests>=2.19.0->datasets==2.16.1) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from requests>=2.19.0->datasets==2.16.1) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from requests>=2.19.0->datasets==2.16.1) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from requests>=2.19.0->datasets==2.16.1) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from pandas->datasets==2.16.1) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from pandas->datasets==2.16.1) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from pandas->datasets==2.16.1) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets==2.16.1) (1.16.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets==2.16.1) (0.2.0)\n",
      "Requirement already satisfied: evaluate in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (0.4.3)\n",
      "Requirement already satisfied: rouge_score in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (0.1.2)\n",
      "Requirement already satisfied: datasets>=2.0.0 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from evaluate) (2.16.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from evaluate) (2.1.2)\n",
      "Requirement already satisfied: dill in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from evaluate) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from evaluate) (4.66.5)\n",
      "Requirement already satisfied: xxhash in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from evaluate) (0.70.15)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2023.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from evaluate) (0.26.0)\n",
      "Requirement already satisfied: packaging in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from evaluate) (24.1)\n",
      "Requirement already satisfied: absl-py in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from rouge_score) (2.1.0)\n",
      "Requirement already satisfied: nltk in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from rouge_score) (3.9.1)\n",
      "Requirement already satisfied: six>=1.14.0 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from rouge_score) (1.16.0)\n",
      "Requirement already satisfied: filelock in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (3.16.1)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (17.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (0.6)\n",
      "Requirement already satisfied: aiohttp in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (3.10.10)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from requests>=2.19.0->evaluate) (2024.8.30)\n",
      "Requirement already satisfied: click in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from nltk->rouge_score) (8.1.7)\n",
      "Requirement already satisfied: joblib in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from nltk->rouge_score) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from nltk->rouge_score) (2024.9.11)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from pandas->evaluate) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from pandas->evaluate) (2024.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.15.5)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages (from yarl<2.0,>=1.12.0->aiohttp->datasets>=2.0.0->evaluate) (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install bitsandbytes\n",
    "!pip install transformers peft accelerate trl\n",
    "!pip install datasets==2.16.1\n",
    "!pip install evaluate rouge_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Setup Global settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indicate availability CUDA devices to help Trainer can recognize and use then in training process\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1,2,3\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "DATASET_NAME = \"FinGPT/fingpt-fiqa_qa\"\n",
    "\n",
    "# load piece data of datasets\n",
    "def get_dataset(from_pc=0, to_pc=10):\n",
    "    dataset_dict = load_dataset(DATASET_NAME, split=\"train[10%:20%]\")\n",
    "    \n",
    "    # rename columns of dataset to fix with format: system_prompt, question, response\n",
    "    dataset_dict = dataset_dict.rename_column(\"instruction\", \"system_prompt\")\n",
    "    dataset_dict = dataset_dict.rename_column(\"input\", \"question\")\n",
    "    dataset_dict = dataset_dict.rename_column(\"output\", \"response\")\n",
    "    \n",
    "    dataset = dataset_dict.train_test_split(test_size=0.1)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def format_instruction(sample):\n",
    "#     return f\"\"\"### System prompt:\n",
    "# {sample['system_prompt']}\n",
    "\n",
    "# ### Question:\n",
    "# {sample[\"question\"]}\n",
    "\n",
    "# ### Response:\n",
    "# {sample[\"response\"]}\n",
    "# \"\"\"\n",
    "\n",
    "def format_instruction(sample):\n",
    "    return f\"\"\"<s>[INST] <<SYS>>{sample[\"system_prompt\"]}<</SYS>>\\n\\nQuestion: {sample[\"question\"]}\\n\\nResponse: {sample[\"response\"]} [/INST]\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'response', 'system_prompt'],\n",
      "        num_rows: 1539\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'response', 'system_prompt'],\n",
      "        num_rows: 172\n",
      "    })\n",
      "})\n",
      "<s>[INST] <<SYS>>Offer your thoughts or opinion on the input financial query or topic using your financial background.<</SYS>>\n",
      "\n",
      "Question: Are stories of turning a few thousands into millions by trading stocks real?\n",
      "\n",
      "Response: The short answer is yes, it is possible to do what these classes claim, however, it is highly unlikely. For every person they can show you that got rich using whatever so called method they are teaching, there are hundreds of people that didn't that they aren't telling you about. What I would recommend is invest in a well diversified portfolio. If you have a higher tolerance for risk then you can make some of that portfolio out of higher risk/reward investments. Maybe you pick the next Apple or Google or Netflix or whatever but that portion of your portfolio should be money that you can afford to lose in case you pick duds. [/INST]\n"
     ]
    }
   ],
   "source": [
    "# Load a piece of data because it is big dataset\n",
    "training_dataset = get_dataset(0, 11)\n",
    "print(training_dataset)\n",
    "\n",
    "# test format instruction\n",
    "example = training_dataset[\"train\"][5]\n",
    "print(format_instruction(example))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline,\n",
    ")\n",
    "\n",
    "from peft import LoraConfig, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "\n",
    "MODEL_NAME = \"AdaptLLM/finance-chat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model():\n",
    "    \"\"\"\n",
    "    Load model in qantization mode 4 big\n",
    "    - https://huggingface.co/docs/accelerate/en/usage_guides/quantization\n",
    "    - https://huggingface.co/blog/4bit-transformers-bitsandbytes\n",
    "    \"\"\"\n",
    "    quant_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "    )\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        quantization_config=quant_config,\n",
    "        device_map = \"auto\",\n",
    "        token=True\n",
    "    )\n",
    "    \n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "    model.config.use_cache = False\n",
    "    model.config.pretraining_tp = 1\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        MODEL_NAME,\n",
    "        token=True,\n",
    "        add_eos_token=True,\n",
    "        add_bos_token=True,\n",
    "        # WARN: Ignore the warning of SFTTrainer for use padding_side=\"right\", using padding side right will cause the model can't generate eos token\n",
    "        padding_side=\"left\",\n",
    "    )\n",
    "    \n",
    "    # https://clay-atlas.com/us/blog/2024/01/01/mistral-sft-trainer-cannot-generate-eos-token/\n",
    "    tokenizer.pad_token = tokenizer.unk_token\n",
    "    \n",
    "    \n",
    "    return model, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27ad3c212c9d47c4b16d1a7253ed4992",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n",
      "  warnings.warn(\n",
      "/usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 0\n",
      "all model parameters: 3500421120\n",
      "percentage of trainable model parameters: 0.00%\n",
      "LlamaTokenizerFast(name_or_path='AdaptLLM/finance-chat', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='left', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<unk>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "def print_number_of_trainable_model_parameters(model):\n",
    "    trainable_model_params = 0\n",
    "    all_model_params = 0\n",
    "\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_params += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_model_params += param.numel()\n",
    "\n",
    "    return f\"trainable model parameters: {trainable_model_params}\\nall model parameters: {all_model_params}\\npercentage of trainable model parameters: {100 * trainable_model_params / all_model_params:.2f}%\"\n",
    "\n",
    "# Clean cache before loading model\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load model and tokenizer\n",
    "model, tokenizer = load_model()\n",
    "\n",
    "# Check number of trainable parameters\n",
    "print(print_number_of_trainable_model_parameters(model))\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Example to chat with the finance-chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### User Input:\n",
      "\n",
      "Input Texts:\n",
      "{Recent indicators suggest that economic activity has continued to expand at a solid pace. Job gains have slowed, and the unemployment rate has moved up but remains low. Inflation has made further progress toward the Committee's 2 percent objective but remains somewhat elevated.\n",
      "The Committee seeks to achieve maximum employment and inflation at the rate of 2 percent over the longer run. The Committee has gained greater confidence that inflation is moving sustainably toward 2 percent, and judges that the risks to achieving its employment and inflation goals are roughly in balance. The economic outlook is uncertain, and the Committee is attentive to the risks to both sides of its dual mandate.\n",
      "In light of the progress on inflation and the balance of risks, the Committee decided to lower the target range for the federal funds rate by 1/2 percentage point to 4-3/4 to 5 percent. In considering additional adjustments to the target range for the federal funds rate, the Committee will carefully assess incoming data, the evolving outlook, and the balance of risks. The Committee will continue reducing its holdings of Treasury securities and agency debt and agency mortgage‑backed securities. The Committee is strongly committed to supporting maximum employment and returning inflation to its 2 percent objective.\n",
      "In assessing the appropriate stance of monetary policy, the Committee will continue to monitor the implications of incoming information for the economic outlook. The Committee would be prepared to adjust the stance of monetary policy as appropriate if risks emerge that could impede the attainment of the Committee's goals. The Committee's assessments will take into account a wide range of information, including readings on labor market conditions, inflation pressures and inflation expectations, and financial and international developments.\n",
      "Based on the provided text, I would classify the overall sentiment and tone as neutral to cautiously optimistic, with an emphasis on ongoing progress and a balanced risk outlook.}\n",
      "\n",
      "Given a list of cleaned text data, conduct a sentiment analysis to evaluate the emotional tone of each text (e.g., positive, neutral, negative). Provide a confidence score for each sentiment classification, as well as a high-level explanation that justifies the analysis. Additionally, assess the potential implications these sentiments may have on the perception of {topic/subject}. Your response should be structured only as follows:\n",
      "\n",
      "\t1.\tSentiment Polarity (Positive, Neutral, Negative)\n",
      "\t2.\tConfidence Score (0 to 9 scale)\n",
      "\n",
      "\n",
      "### Assistant Output:\n",
      "1. Sentiment Polarity: Neutral to cautiously optimistic\n",
      "2. Confidence Score: 6\n",
      "\n",
      "Explanation: The overall sentiment of the text is neutral to cautiously optimistic, as it acknowledges progress on inflation and the balance of risks, while also emphasizing the need to continue monitoring the economic outlook and adjusting monetary policy as appropriate. The confidence score of 6 indicates that the sentiment is somewhat positive, but not overly optimistic. The potential implications of this sentiment on the perception of the topic are that it suggests a balanced approach to monetary policy and a cautious outlook for the economy.\n"
     ]
    }
   ],
   "source": [
    "user_input = \"\"\"\n",
    "Input Texts:\n",
    "{Recent indicators suggest that economic activity has continued to expand at a solid pace. Job gains have slowed, and the unemployment rate has moved up but remains low. Inflation has made further progress toward the Committee's 2 percent objective but remains somewhat elevated.\n",
    "The Committee seeks to achieve maximum employment and inflation at the rate of 2 percent over the longer run. The Committee has gained greater confidence that inflation is moving sustainably toward 2 percent, and judges that the risks to achieving its employment and inflation goals are roughly in balance. The economic outlook is uncertain, and the Committee is attentive to the risks to both sides of its dual mandate.\n",
    "In light of the progress on inflation and the balance of risks, the Committee decided to lower the target range for the federal funds rate by 1/2 percentage point to 4-3/4 to 5 percent. In considering additional adjustments to the target range for the federal funds rate, the Committee will carefully assess incoming data, the evolving outlook, and the balance of risks. The Committee will continue reducing its holdings of Treasury securities and agency debt and agency mortgage‑backed securities. The Committee is strongly committed to supporting maximum employment and returning inflation to its 2 percent objective.\n",
    "In assessing the appropriate stance of monetary policy, the Committee will continue to monitor the implications of incoming information for the economic outlook. The Committee would be prepared to adjust the stance of monetary policy as appropriate if risks emerge that could impede the attainment of the Committee's goals. The Committee's assessments will take into account a wide range of information, including readings on labor market conditions, inflation pressures and inflation expectations, and financial and international developments.\n",
    "Based on the provided text, I would classify the overall sentiment and tone as neutral to cautiously optimistic, with an emphasis on ongoing progress and a balanced risk outlook.}\n",
    "\n",
    "Given a list of cleaned text data, conduct a sentiment analysis to evaluate the emotional tone of each text (e.g., positive, neutral, negative). Provide a confidence score for each sentiment classification, as well as a high-level explanation that justifies the analysis. Additionally, assess the potential implications these sentiments may have on the perception of {topic/subject}. Your response should be structured only as follows:\n",
    "\n",
    "\t1.\tSentiment Polarity (Positive, Neutral, Negative)\n",
    "\t2.\tConfidence Score (0 to 9 scale)\n",
    "\"\"\"\n",
    "\n",
    "# Apply the prompt template and system prompt of LLaMA-2-Chat demo for chat models (NOTE: NO prompt template is required for base models!)\n",
    "our_system_prompt = \"\\nYou are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\\n\" # Please do NOT change this\n",
    "prompt = f\"<s>[INST] <<SYS>>{our_system_prompt}<</SYS>>\\n\\n{user_input} [/INST]\"\n",
    "\n",
    "# # NOTE:\n",
    "# # If you want to apply your own system prompt, please integrate it into the instruction part following our system prompt like this:\n",
    "# your_system_prompt = \"Please, check if the answer can be inferred from the pieces of context provided.\"\n",
    "# prompt = f\"<s>[INST] <<SYS>>{our_system_prompt}<</SYS>>\\n\\n{your_system_prompt}\\n{user_input} [/INST]\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(model.device)\n",
    "outputs = model.generate(input_ids=inputs, max_length=4096)[0]\n",
    "\n",
    "answer_start = int(inputs.shape[-1])\n",
    "pred = tokenizer.decode(outputs[answer_start:], skip_special_tokens=True)\n",
    "\n",
    "print(f'### User Input:\\n{user_input}\\n\\n### Assistant Output:\\n{pred}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### User Input:\n",
      "Are COBRA premiums deductible when self-employed?\n",
      "\n",
      "### Assistant Output:\n",
      "As an AI language model, I do not have personal financial advice or opinions. However, I can provide you with general information on the topic.\n",
      "\n",
      "COBRA premiums are typically not deductible for self-employed individuals. This is because COBRA is a federal law that requires employers to offer continuation of health insurance coverage to employees who lose their job or experience a reduction in hours. Self-employed individuals are not considered employees, so they are not eligible for COBRA coverage.\n",
      "\n",
      "However, there are some exceptions to this rule. If you are self-employed and have a spouse or dependent child who is covered under your employer's health insurance plan, you may be eligible for COBRA coverage if you lose your job or experience a reduction in hours. Additionally, if you are self-employed and have a business partner who is an employee of your business, you may be eligible for COBRA coverage if your partner loses their job or experiences a reduction in hours.\n",
      "\n",
      "It's important to note that COBRA coverage is only available for a limited time, typically 18 months. After that, you may be eligible for other types of health insurance coverage, such as a private insurance plan or a government-sponsored plan like Medicare or Medicaid.\n",
      "\n",
      "In summary, while COBRA premiums are not typically deductible for self-employed individuals, there may be exceptions for spouses or dependents covered under your employer's plan or business partners who are employees of your business. It's important to consult with a tax professional or financial advisor to determine your specific eligibility for COBRA coverage and any potential tax implications.\n"
     ]
    }
   ],
   "source": [
    "test = training_dataset[\"train\"][1]\n",
    "user_input = test[\"question\"]\n",
    "\n",
    "# Apply the prompt template and system prompt of LLaMA-2-Chat demo for chat models (NOTE: NO prompt template is required for base models!)\n",
    "our_system_prompt = test[\"system_prompt\"]\n",
    "prompt = f\"<s>[INST] <<SYS>>{our_system_prompt}<</SYS>>\\n\\n{user_input} [/INST]\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(model.device)\n",
    "outputs = model.generate(input_ids=inputs, max_length=4096)[0]\n",
    "\n",
    "answer_start = int(inputs.shape[-1])\n",
    "pred = tokenizer.decode(outputs[answer_start:], skip_special_tokens=True)\n",
    "\n",
    "print(f'### User Input:\\n{user_input}\\n\\n### Assistant Output:\\n{pred}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. LoRA configuration and Training Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Lora\n",
    "lora_config = LoraConfig(\n",
    "    # Lora attention dimension\n",
    "    r=64,\n",
    "    # Scaling process\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    # Prevent overfitting\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "\n",
    "LEARNING_RATE = 1e-5 # 2e-4\n",
    "WEIGHT_DECAY=0.001\n",
    "EPOCHS = 3\n",
    "BATCH_SIZE = 4\n",
    "LOGGING_STEPS = 10\n",
    "MAX_SEQ_LEN = 2048\n",
    "MAX_STEPS = -1\n",
    "\n",
    "TRAINING_OUTPUT_DIR=f\"outputs/peft-financial-chatbot-trained-{str(int(time.time()))}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages/transformers/training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:292: UserWarning: You didn't pass a `max_seq_length` argument to the SFTTrainer, this will default to 1024\n",
      "  warnings.warn(\n",
      "/usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0680dc832d04ef89cb77bb7ac89cea2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1539 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5da6a46bcaa84281a9ae136bd7f0a386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/172 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:396: UserWarning: You passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to overflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = 'right'` to your code.\n",
      "  warnings.warn(\n",
      "/usr1/home/s124mdg41_08/miniconda3/envs/myenv/lib/python3.12/site-packages/trl/trainer/sft_trainer.py:401: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=TRAINING_OUTPUT_DIR,\n",
    "    \n",
    "    num_train_epochs=EPOCHS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    max_steps=MAX_STEPS,\n",
    "    logging_steps=LOGGING_STEPS,\n",
    "    \n",
    "    # max_grad_norm=0.3, # measure of the magnitude or steepness of the gradient of a loss function\n",
    "    warmup_ratio=0.03,\n",
    "    lr_scheduler_type=\"constant\",\n",
    "    optim=\"paged_adamw_8bit\",\n",
    "    \n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    gradient_accumulation_steps=BATCH_SIZE,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=LOGGING_STEPS,\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    \n",
    "    peft_config=lora_config,\n",
    "    args=training_args,\n",
    "    \n",
    "    train_dataset=training_dataset[\"train\"],\n",
    "    eval_dataset=training_dataset[\"test\"],\n",
    "    dataset_text_field=\"question\",\n",
    "    \n",
    "    max_seq_length=None,\n",
    "    formatting_func=format_instruction,\n",
    "#     packing=True\n",
    ")\n",
    "\n",
    "print(training_args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='288' max='288' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [288/288 42:21, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.137300</td>\n",
       "      <td>4.199275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.151200</td>\n",
       "      <td>4.199275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>4.105700</td>\n",
       "      <td>4.199275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>4.047200</td>\n",
       "      <td>4.199275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.177300</td>\n",
       "      <td>4.199275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>4.076700</td>\n",
       "      <td>4.199275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>4.083900</td>\n",
       "      <td>4.199275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>4.181700</td>\n",
       "      <td>4.199275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>4.175800</td>\n",
       "      <td>4.199275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.539700</td>\n",
       "      <td>4.199275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>4.180100</td>\n",
       "      <td>4.199275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>4.023500</td>\n",
       "      <td>4.199275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>4.109000</td>\n",
       "      <td>4.199275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>4.084300</td>\n",
       "      <td>4.199275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>4.226300</td>\n",
       "      <td>4.199275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>4.084600</td>\n",
       "      <td>4.199275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>4.134000</td>\n",
       "      <td>4.199275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>4.181000</td>\n",
       "      <td>4.199275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>4.134100</td>\n",
       "      <td>4.199275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.364800</td>\n",
       "      <td>4.199275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>4.095700</td>\n",
       "      <td>4.199275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>4.137600</td>\n",
       "      <td>4.199275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>4.158200</td>\n",
       "      <td>4.199275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>4.144900</td>\n",
       "      <td>4.199275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>4.269700</td>\n",
       "      <td>4.199275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>4.123100</td>\n",
       "      <td>4.199275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>4.121100</td>\n",
       "      <td>4.199275</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>4.191800</td>\n",
       "      <td>4.199275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./outputs/peft-training-checkpoint/tokenizer_config.json',\n",
       " './outputs/peft-training-checkpoint/special_tokens_map.json',\n",
       " './outputs/peft-training-checkpoint/tokenizer.model',\n",
       " './outputs/peft-training-checkpoint/added_tokens.json',\n",
       " './outputs/peft-training-checkpoint/tokenizer.json')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PEFT_MODEL_LOCAL_CHECKPOINT = \"./outputs/peft-training-checkpoint\"\n",
    "PEFT_MODEL_ADAPTER_ID = \"anhtranhong/finance-chat_fingpt-fiqa_qa_v2\"\n",
    "trainer.train()\n",
    "trainer.model.save_pretrained(PEFT_MODEL_LOCAL_CHECKPOINT)\n",
    "tokenizer.save_pretrained(PEFT_MODEL_LOCAL_CHECKPOINT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 162218048\n",
      "all model parameters: 3662639168\n",
      "percentage of trainable model parameters: 4.43%\n"
     ]
    }
   ],
   "source": [
    "print(print_number_of_trainable_model_parameters(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Push to huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Model generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import PeftModel\n",
    "\n",
    "peft_model = PeftModel.from_pretrained(model, PEFT_MODEL_ADAPTER_ID, is_trainable=True)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    PEFT_MODEL_ADAPTER_ID,\n",
    "    padding_side=\"left\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Date                                    Minutes_cleaned\n",
      "0   2012-01-25       Minutes of the Federal Open Market Committee\n",
      "1   2012-01-25  A meeting of the Federal Open Market Committee...\n",
      "2   2012-01-25  Role of Financial Conditions in Economic Recov...\n",
      "3   2012-01-25  Staff summarized research projects being condu...\n",
      "4   2012-01-25  In their discussion following the staff presen...\n",
      "5   2012-01-25  In the agenda for this meeting, it was reporte...\n",
      "6   2012-01-25  The elected members and alternate members were...\n",
      "7   2012-01-25  By unanimous vote, the following officers of t...\n",
      "8   2012-01-25  By unanimous vote, the Authorization for Domes...\n",
      "9   2012-01-25  AUTHORIZATION FOR DOMESTIC OPEN MARKET OPERATIONS\n",
      "10  2012-01-25  A. To buy or sell U.S. Government securities, ...\n",
      "11  2012-01-25  B. To buy or sell in the open market U.S. Gove...\n",
      "12  2012-01-25  A. for System Open Market Account, to sell U.S...\n",
      "13  2012-01-25  B. for New York Bank account, when appropriate...\n",
      "14  2012-01-25  Transactions undertaken with such accounts und...\n",
      "15  2012-01-25  5. In the execution of the Committee's decisio...\n",
      "16  2012-01-25      AUTHORIZATION FOR FOREIGN CURRENCY OPERATIONS\n",
      "17  2012-01-25  B. To hold balances of, and to have outstandin...\n",
      "18  2012-01-25  C. To draw foreign currencies and to permit fo...\n",
      "19  2012-01-25  D. To maintain an overall open position in all...\n",
      "20  2012-01-25  Any changes in the terms of existing swap arra...\n",
      "21  2012-01-25  3. All transactions in foreign currencies unde...\n",
      "22  2012-01-25  5. Foreign currency holdings shall be invested...\n",
      "23  2012-01-25  6. All operations undertaken pursuant to the p...\n",
      "24  2012-01-25  A. With the approval of the Committee, to ente...\n",
      "25  2012-01-25  B. To keep the Secretary of the Treasury fully...\n",
      "26  2012-01-25  8. Staff officers of the Committee are authori...\n",
      "27  2012-01-25  1. System operations in foreign currencies sha...\n",
      "28  2012-01-25           2. To achieve this end the System shall:\n",
      "29  2012-01-25  A. Undertake spot and forward purchases and sa...\n",
      "30  2012-01-25  B. Maintain reciprocal currency (\"swap\") arran...\n",
      "31  2012-01-25  C. Cooperate in other respects with central ba...\n",
      "32  2012-01-25  A. To adjust System balances in light of proba...\n",
      "33  2012-01-25  C. For such other purposes as may be expressly...\n",
      "34  2012-01-25  4. System foreign currency operations shall be...\n",
      "35  2012-01-25  B. In cooperation, as appropriate, with foreig...\n",
      "36  2012-01-25  C. In a manner consistent with the obligations...\n",
      "37  2012-01-25  PROCEDURAL INSTRUCTIONS WITH RESPECT TO FOREIG...\n",
      "38  2012-01-25  1. The Manager shall clear with the Subcommitt...\n",
      "39  2012-01-25  A. Any operation that would result in a change...\n",
      "40  2012-01-25  B. Any operation that would result in a change...\n",
      "41  2012-01-25  C. Any operation that might generate a substan...\n",
      "42  2012-01-25  D. Any swap drawing proposed by a foreign bank...\n",
      "43  2012-01-25  2. The Manager shall clear with the Committee ...\n",
      "44  2012-01-25  A. Any operation that would result in a change...\n",
      "45  2012-01-25  B. Any swap drawing proposed by a foreign bank...\n",
      "46  2012-01-25  3. The Manager shall also consult with the Sub...\n",
      "47  2012-01-25  Statement on Longer-Run Goals and Monetary Pol...\n",
      "48  2012-01-25  Following the Committee's disposition of organ...\n",
      "49  2012-01-25  In presenting the draft statement on behalf of...\n",
      "7181\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = '/usr1/home/s124mdg41_08/FinLLM-FOMC/data/processed/Merged_FMOC.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Function to split each note by paragraphs and remove paragraphs with less than 50 characters\n",
    "def split_into_paragraphs(row):\n",
    "    date = row['Date']\n",
    "    paragraphs = str(row['Minutes_cleaned']).split('\\n')\n",
    "    # Only keep paragraphs that are non-empty and have at least 50 characters (ignoring spaces)\n",
    "    return [(date, para.strip()) for para in paragraphs if para.strip() and len(para.strip()) >= 40]\n",
    "\n",
    "# Apply the function to each row in the DataFrame\n",
    "split_data = [item for idx, row in df.iterrows() for item in split_into_paragraphs(row)]\n",
    "\n",
    "# Create a new DataFrame with the split paragraphs\n",
    "new_df = pd.DataFrame(split_data, columns=['Date', 'Minutes_cleaned'])\n",
    "\n",
    "# 删除 'Minutes_cleaned' 列中为空字符串或者'NaN' 的行\n",
    "new_df = new_df[new_df['Minutes_cleaned'].notna() & (new_df['Minutes_cleaned'] != '')]\n",
    "\n",
    "# 输出前50行并输出行数\n",
    "print(new_df.head(50))\n",
    "print(len(new_df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(7, 8), match='6'>\n",
      "<re.Match object; span=(7, 8), match='5'>\n",
      "<re.Match object; span=(7, 8), match='7'>\n",
      "<re.Match object; span=(7, 8), match='7'>\n",
      "<re.Match object; span=(77, 78), match='7'>\n",
      "<re.Match object; span=(7, 8), match='6'>\n",
      "<re.Match object; span=(17, 18), match='5'>\n",
      "<re.Match object; span=(167, 168), match='5'>\n",
      "<re.Match object; span=(7, 8), match='6'>\n",
      "<re.Match object; span=(61, 62), match='7'>\n",
      "None\n",
      "None\n",
      "<re.Match object; span=(71, 72), match='7'>\n",
      "None\n",
      "<re.Match object; span=(66, 67), match='5'>\n",
      "<re.Match object; span=(17, 18), match='4'>\n",
      "<re.Match object; span=(186, 187), match='5'>\n",
      "<re.Match object; span=(78, 79), match='5'>\n",
      "None\n",
      "<re.Match object; span=(26, 27), match='6'>\n",
      "<re.Match object; span=(183, 184), match='5'>\n",
      "<re.Match object; span=(26, 27), match='5'>\n",
      "<re.Match object; span=(129, 130), match='7'>\n",
      "None\n",
      "<re.Match object; span=(67, 68), match='4'>\n",
      "<re.Match object; span=(7, 8), match='3'>\n",
      "<re.Match object; span=(26, 27), match='5'>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# 生成情感分数\u001b[39;00m\n\u001b[1;32m     23\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, add_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mto(model\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 25\u001b[0m outputs \u001b[38;5;241m=\u001b[39m peft_model\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m     26\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m     27\u001b[0m     do_sample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     28\u001b[0m     max_new_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m,\n\u001b[1;32m     29\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.6\u001b[39m,\n\u001b[1;32m     30\u001b[0m     top_p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m,\n\u001b[1;32m     31\u001b[0m     top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[1;32m     32\u001b[0m     repetition_penalty\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.2\u001b[39m,\n\u001b[1;32m     33\u001b[0m     num_return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     34\u001b[0m )[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     36\u001b[0m \u001b[38;5;66;03m# 解码并提取分数\u001b[39;00m\n\u001b[1;32m     37\u001b[0m answer_start \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/peft/peft_model.py:977\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.generate\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgeneration_config \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneration_config\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 977\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    978\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[1;32m    979\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbase_model_prepare_inputs_for_generation\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/transformers/generation/utils.py:2215\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   2207\u001b[0m     input_ids, model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   2208\u001b[0m         input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m   2209\u001b[0m         expand_size\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   2210\u001b[0m         is_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   2211\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2212\u001b[0m     )\n\u001b[1;32m   2214\u001b[0m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[0;32m-> 2215\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sample(\n\u001b[1;32m   2216\u001b[0m         input_ids,\n\u001b[1;32m   2217\u001b[0m         logits_processor\u001b[38;5;241m=\u001b[39mprepared_logits_processor,\n\u001b[1;32m   2218\u001b[0m         stopping_criteria\u001b[38;5;241m=\u001b[39mprepared_stopping_criteria,\n\u001b[1;32m   2219\u001b[0m         generation_config\u001b[38;5;241m=\u001b[39mgeneration_config,\n\u001b[1;32m   2220\u001b[0m         synced_gpus\u001b[38;5;241m=\u001b[39msynced_gpus,\n\u001b[1;32m   2221\u001b[0m         streamer\u001b[38;5;241m=\u001b[39mstreamer,\n\u001b[1;32m   2222\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   2223\u001b[0m     )\n\u001b[1;32m   2225\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SAMPLE, GenerationMode\u001b[38;5;241m.\u001b[39mBEAM_SEARCH):\n\u001b[1;32m   2226\u001b[0m     \u001b[38;5;66;03m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   2227\u001b[0m     beam_scorer \u001b[38;5;241m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   2228\u001b[0m         batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   2229\u001b[0m         num_beams\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2234\u001b[0m         max_length\u001b[38;5;241m=\u001b[39mgeneration_config\u001b[38;5;241m.\u001b[39mmax_length,\n\u001b[1;32m   2235\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/myenv/lib/python3.12/site-packages/transformers/generation/utils.py:3195\u001b[0m, in \u001b[0;36mGenerationMixin._sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   3192\u001b[0m unfinished_sequences \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(batch_size, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m   3193\u001b[0m model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_initial_cache_position(input_ids, model_kwargs)\n\u001b[0;32m-> 3195\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_unfinished_sequences(\n\u001b[1;32m   3196\u001b[0m     this_peer_finished, synced_gpus, device\u001b[38;5;241m=\u001b[39minput_ids\u001b[38;5;241m.\u001b[39mdevice, cur_len\u001b[38;5;241m=\u001b[39mcur_len, max_length\u001b[38;5;241m=\u001b[39mmax_length\n\u001b[1;32m   3197\u001b[0m ):\n\u001b[1;32m   3198\u001b[0m     \u001b[38;5;66;03m# prepare model inputs\u001b[39;00m\n\u001b[1;32m   3199\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   3201\u001b[0m     \u001b[38;5;66;03m# prepare variable output controls (note: some models won't accept all output controls)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# 初始化一个空列表来保存每一行的结果\n",
    "results = []\n",
    "\n",
    "# 遍历 new_df 的每一行\n",
    "for idx, row in new_df.iterrows():\n",
    "    content = row['Minutes_cleaned']  # 获取 'Minutes_cleaned' 列的内容\n",
    "    date_value = row['Date']  # 获取 'Date' 列的值\n",
    "\n",
    "    our_system_prompt = \"\"\"You're an expert on sentiment analysis in economic texts\"\"\"\n",
    "\n",
    "    user_input = f\"\"\"Please analyze the economic sentiment of the following content and provide a score from 0 to 10, where 0 represents extremely negative sentiment, 5 is neutral, and 10 is extremely positive. Analyze only the economic-related aspects in each paragraph. If you can't judge the emotional content, judge it as 5.\n",
    "        Content: {content}\n",
    "        <requirement>Only show number of one score, no more text.</requirement>\n",
    "    \"\"\"\n",
    "    \n",
    "    # 构建最终的 LLaMA2 prompt\n",
    "    prompt = f\"<s>[INST] <<SYS>>{our_system_prompt}<</SYS>>\\n\\n{user_input} [/INST]\"\n",
    "\n",
    "    # 生成情感分数\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).input_ids.to(model.device)\n",
    "\n",
    "    outputs = peft_model.generate(\n",
    "        input_ids=inputs,\n",
    "        do_sample=True,\n",
    "        max_new_tokens=1024,\n",
    "        temperature=0.6,\n",
    "        top_p=0.9,\n",
    "        top_k=50,\n",
    "        repetition_penalty=1.2,\n",
    "        num_return_sequences=1,\n",
    "    )[0]\n",
    "\n",
    "    # 解码并提取分数\n",
    "    answer_start = int(inputs.shape[-1])\n",
    "    pred = tokenizer.decode(outputs[answer_start:], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "   # 使用 re.search() 获取第一个匹配的数字\n",
    "    score = re.search(r'\\d+', pred)\n",
    "    print(score)\n",
    "\n",
    "    # 处理提取的分数\n",
    "    if score:\n",
    "        score_value = int(score[0])  # 提取第一个匹配的分数\n",
    "    else:\n",
    "        score_value = None  # 如果没有找到分数，设置为 None\n",
    "\n",
    "    # 将结果与 Date 合并，存入结果列表\n",
    "    results.append({\n",
    "        'Date': date_value,\n",
    "        'Sentiment_Score': score_value\n",
    "    })\n",
    "    \n",
    "\n",
    "# 将结果列表转换为 DataFrame\n",
    "result_df = pd.DataFrame(results)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = '/usr1/home/s124mdg41_08/FinLLM-FOMC/data/sentiment_score/sentiment_analysis_results.csv'\n",
    "result_df.to_csv(csv_file_path, index=False)\n",
    "\n",
    "print(f\"Results saved to {csv_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
